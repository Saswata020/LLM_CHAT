{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyIe7OM/YZwDX/dY8LoNKq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saswata020/LLM_CHAT/blob/main/Finance_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Importing the 'os' module for operating system-related functionality\n",
        "import streamlit as st  # Importing the 'streamlit' library and naming it as 'st' for creating web applications\n",
        "import pickle  # Importing 'pickle' for serializing and deserializing Python objects\n",
        "import time  # Importing the 'time' module for time-related functions\n",
        "from langchain import OpenAI  # Importing 'OpenAI' from the 'langchain' package\n",
        "from langchain.chains import RetrievalQAWithSourcesChain  # Importing 'RetrievalQAWithSourcesChain' from 'langchain.chains'\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Importing 'RecursiveCharacterTextSplitter' for text splitting\n",
        "from langchain.document_loaders import UnstructuredURLLoader  # Importing 'UnstructuredURLLoader' for loading data from URLs\n",
        "from langchain.embeddings import OpenAIEmbeddings  # Importing 'OpenAIEmbeddings' for embeddings\n",
        "from langchain.vectorstores import FAISS  # Importing 'FAISS' for vector storage\n",
        "\n",
        "from dotenv import load_dotenv  # Importing 'load_dotenv' from 'dotenv' to load environment variables\n",
        "load_dotenv()  # Loading environment variables from a .env file (e.g., an OpenAI API key)\n",
        "\n",
        "st.title(\"RockyBot: News Research Tool ðŸ“ˆ\")  # Setting the title for the web application\n",
        "st.sidebar.title(\"News Article URLs\")  # Setting the title for the sidebar\n",
        "\n",
        "urls = []  # Initializing an empty list to store URLs\n",
        "for i in range(3):  # Iterating three times to get URLs from the user via the sidebar\n",
        "    url = st.sidebar.text_input(f\"URL {i+1}\")  # Getting input URLs from the user\n",
        "    urls.append(url)  # Appending the URLs to the list\n",
        "\n",
        "process_url_clicked = st.sidebar.button(\"Process URLs\")  # Creating a button to trigger URL processing\n",
        "\n",
        "file_path = \"faiss_store_openai.pkl\"  # Setting the file path for storing FAISS index\n",
        "\n",
        "main_placeholder = st.empty()  # Creating a placeholder for updating messages dynamically\n",
        "llm = OpenAI(temperature=0.9, max_tokens=100)  # Initializing OpenAI with certain parameters\n",
        "\n",
        "if process_url_clicked:  # Checking if the 'Process URLs' button is clicked\n",
        "    loader = UnstructuredURLLoader(urls=urls)  # Loading data from the provided URLs\n",
        "    main_placeholder.text(\"Data Loading...Started...âœ…âœ…âœ…\")  # Displaying a message for data loading\n",
        "    data = loader.load()  # Loading the data from the URLs\n",
        "    text_splitter = RecursiveCharacterTextSplitter(separators=['\\n\\n', '\\n', '.', ','], chunk_size=1000)  # Initializing text splitter\n",
        "    main_placeholder.text(\"Text Splitter...Started...âœ…âœ…âœ…\")  # Displaying a message for text splitting\n",
        "    docs = text_splitter.split_documents(data)  # Splitting the data into documents\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()  # Creating embeddings\n",
        "    vectorstore_openai = FAISS.from_documents(docs, embeddings)  # Building a FAISS index with the documents\n",
        "    main_placeholder.text(\"Embedding Vector Started Building...âœ…âœ…âœ…\")  # Displaying a message for building vectors\n",
        "    time.sleep(2)  # Introducing a delay for 2 seconds\n",
        "\n",
        "    with open(file_path, \"wb\") as f:  # Saving the FAISS index to a pickle file\n",
        "        pickle.dump(vectorstore_openai, f)\n",
        "\n",
        "query = main_placeholder.text_input(\"Question: \")  # Getting user input for a question\n",
        "if query:  # Checking if the user has input a question\n",
        "    if os.path.exists(file_path):  # Checking if the FAISS index file exists\n",
        "        with open(file_path, \"rb\") as f:  # Opening the FAISS index file\n",
        "            vectorstore = pickle.load(f)  # Loading the FAISS index\n",
        "            chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())  # Creating a chain for question-answering\n",
        "            result = chain({\"question\": query}, return_only_outputs=True)  # Getting the answer from the chain\n",
        "\n",
        "            st.header(\"Answer\")  # Displaying the answer header\n",
        "            st.write(result[\"answer\"])  # Showing the answer\n",
        "\n",
        "            sources = result.get(\"sources\", \"\")  # Getting sources, if available\n",
        "            if sources:  # Checking if sources are available\n",
        "                st.subheader(\"Sources:\")  # Displaying a header for sources\n",
        "                sources_list = sources.split(\"\\n\")  # Splitting the sources by newline\n",
        "                for source in sources_list:  # Iterating through the sources\n",
        "                    st.write(source)  # Displaying each source\n"
      ],
      "metadata": {
        "id": "jbmX80Jc0Mq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ix_FRA2Ewo7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acxo6MSZwo3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJacu10uwo0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U7t19wBjwoyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oV2Xbm8wowh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cd C:\\Users\\SASWATA\\PycharmProjects\\LLM\n",
        "\n",
        "\n",
        "#python C:\\Users\\SASWATA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\streamlit run main.py"
      ],
      "metadata": {
        "id": "jGF1gHLUwoth"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}